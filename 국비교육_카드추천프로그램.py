# -*- coding: utf-8 -*-
"""국비교육_카드추천프로그램.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zR1kg2vMft6q1cXTy9BaE-ZpWTr01y6C

### 코사인유사도
### https://wikidocs.net/24603
"""

pip install selenium

pip install selenium webdriver-manager

"""## 1. 아리마 모델

### 2-1. 데이터통합
"""

# path = r'C:\ITWILL'
# pattern = pd.read_csv(path + '/블록별 성별연령대별 카드소비패턴.csv')
# code = pd.read_csv(path + '/카드소비 업종코드.csv')

import pandas as pd
pattern = pd.read_csv('블록별 성별연령대별 카드소비패턴.csv',encoding='cp949')
code = pd.read_csv('카드소비 업종코드.csv' , encoding='UTF8')

pattern.info()
'''
 0   서울시민업종코드(UPJONG_CD)  500 non-null    object
 1   기준년월(YM)             500 non-null    int64
 2   고객주소블록코드(BLOCK_CD)   500 non-null    int64
 3   성별(GEDNER)           500 non-null    object
 4   연령대별(AGE)            500 non-null    object
 5   카드이용금액계(AMT_CORR)    500 non-null    int64
 6   카드이용건수계(USECT_CORR)  500 non-null    int64
'''

pattern=pattern.drop('고객주소블록코드(BLOCK_CD)', axis=1)

pattern.info()
'''
 0   서울시민업종코드(UPJONG_CD)  500 non-null    object
 1   기준년월(YM)             500 non-null    int64
 2   성별(GEDNER)           500 non-null    object
 3   연령대별(AGE)            500 non-null    object
 4   카드이용금액계(AMT_CORR)    500 non-null    int64
 5   카드이용건수계(USECT_CORR)  500 non-null   int64
'''

pattern = pattern.rename(columns={'서울시민업종코드(UPJONG_CD)':'UPJONG_CD'})
pattern = pattern.rename(columns={'기준년월(YM)':'YM'})
pattern = pattern.rename(columns={'성별(GEDNER)':'GEDNER'})
pattern = pattern.rename(columns={'연령대별(AGE)':'AGE'})
pattern = pattern.rename(columns={'카드이용금액계(AMT_CORR)':'AMT_CORR'})
pattern = pattern.rename(columns={'카드이용건수계(USECT_CORR)':'USECT_CORR'})

pattern.info()
'''
0   UPJONG_CD   500 non-null    object
1   YM          500 non-null    int64
2   GEDNER      500 non-null    object
3   AGE         500 non-null    object
4   AMT_CORR    500 non-null    int64
5   USECT_CORR  500 non-null    int64
'''

code.info()
'''
 0   업종코드(UPJONG_CD)  75 non-null     object
 1   대분류(CLASS1)      75 non-null     object
 2   중분류(CLASS2)      75 non-null     object
 3   소분류(CLASS3)      75 non-null
'''

code = code.rename(columns={'업종코드(UPJONG_CD)':'UPJONG_CD'})
code = code.rename(columns={'대분류(CLASS1)':'CLASS1'})
code = code.rename(columns={'중분류(CLASS2)':'CLASS2'})
code = code.rename(columns={'소분류(CLASS3)':'CLASS3'})

pattern.info()
code.info()
'''
0   UPJONG_CD  75 non-null     object
1   CLASS1     75 non-null     object
2   CLASS2     75 non-null     object
3   CLASS3     75 non-null     object
'''
code['UPJONG_CD'] = code['UPJONG_CD'].str.upper()

pattern.info()
code.info()

merged_df = pd.merge(pattern, code, on='UPJONG_CD', how='left')
merged_df

"""## 2-2. 아리마모델"""

# -*- coding: utf-8 -*-
"""
코드 진행

파일 불러오기
데이터 전처리
ARIMA 모델에 맞게 입력 데이터 형태 변환 : arima_input()
데이터 정상성 확인 : kpss_test()
ARIMA 모델 생성 : arima_model()
SARIMAX 모델 생성
"""
import pandas as pd
import itertools
from tqdm import tqdm  # 진행상태 프로그래스바로 출력
import warnings
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import kpss # 데이터 정상성 검증
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)

##############################
## 1. 파일 불러오기
##############################

# 소비내역파일 불러오기
#file = pd.read_csv(r'C:\Study\Python\final_proj\data/카드소비패턴_sample.csv', encoding='euc-kr', header=0)
# file = pd.read_csv(r'\\\final_project/random_sample2.csv')
file= pd.read_csv('카드소비패턴_병합.csv',encoding='cp949')

file.info()
'''
Data columns (total 10 columns):
 #   Column    Non-Null Count  Dtype
---  ------    --------------  -----
 0   서울시민업종코드  39999 non-null  object
 1   대분류       39999 non-null  object
 2   중분류       39999 non-null  object
 3   소분류       39999 non-null  object
 4   기준년월      39999 non-null  int64
 5   고객주소블록코드  39999 non-null  int64
 6   성별        39999 non-null  object
 7   연령대별      39999 non-null  object
 8   카드이용금액계   39999 non-null  int64
 9   카드이용건수계   39999 non-null  int64
'''

# 업종코드 불러오기
# code = pd.read_csv(r'\\\final_project/서울시민 카드소비 업종코드.csv')
code= pd.read_csv('카드소비 업종코드.csv')
code.info()
'''
 #   Column           Non-Null Count  Dtype
---  ------           --------------  -----
 0   UPJONG_CD  75 non-null     object
 1   CLASS1      75 non-null     object
 2   CLASS2      75 non-null     object
 3   CLASS3      75 non-null     object
'''
'''
업종코드(UPJONG_CD), 대분류(CLASS1), 중분류(CLASS2), 소분류(CLASS3)

'''

##############################
## 2. 예측모델 생성 전 데이터 전처리
##############################

# 소비데이터 copy
df = file.copy()

# code 업종코드 ss001 -> SS001 변환
# code['UPJONG_CD'] = code['UPJONG_CD'].apply(str.upper)
code['업종코드(UPJONG_CD)'] = code['업종코드(UPJONG_CD)'].apply(str.upper)

# 업종코드, 소분류 각각 범주 확인
# up_code = sorted(df['서울시민업종코드'].unique())
up_code = sorted(df['업종코드'].unique())
class3 = code['소분류(CLASS3)'].unique() # class3 = code['CLASS3'].unique()

print(len(up_code)) # 75  , 65
print(len(class3)) # 75  ,75

up_code = up_code[:65]  # up_code를 65개로 맞춤
class3 = class3[:65]    # class3를 65개로 맞춤

'''
# 소비내역파일 업종코드에 해당하는 소분류 추출
up_class3 = []

for i in range(len(class3)):
    if class3[i] in up_code:
        up_class3.append(code['소분류(CLASS3)'][i])

len(up_class3) # 65
'''

# 서울시민업종코드 -> 소분류 replace
# df['서울시민업종코드'].replace(to_replace=up_code, value=class3, inplace=True)
df['업종코드'].replace(to_replace=up_code, value=class3, inplace=True)

# 고객주소블록코드(BLOCK_CD) 제거
# df.drop('고객주소블록코드', axis=1, inplace=True) --> 데이터통합때 이미 드랍함


# 날짜데이터 기간 확인
df['기준년월'].min() # '202101'
df['기준년월'].max() # '202208'

# 2030 추출
df_2030 = df[(df['연령대별'] == '20대') | (df['연령대별'] == '30대') ] # [15496 rows x 9 columns]

'''
target_index = pd.date_range('20160101', '20210731', freq='M').to_period('M')
len(target_index) # 67

target = target_index.to_frame()
target.columns = ['기준년월(YM)']

# 카테고리별 관측치 많은 순서대로 top10 선정 - 내림차순으로 index만 저장
grp_top10 = grouped_data.value_counts('카드이용건수계')
'''
# 소분류, 기준년월에 따라 그룹화 및 sum()
# grouped_data = df_2030.groupby(['서울시민업종코드', '기준년월']).sum(['카드이용금액계', '카드이용건수계']).reset_index()
grouped_data = df_2030.groupby(['업종코드', '기준년월']).sum(['카드이용금액계', '카드이용건수계']).reset_index()
grouped_data = grouped_data.sort_values('카드이용금액계', ascending=False) # [1301 rows x 4 columns]

# 카드이용건수계 많은 순서대로 top10 선정
# n_grp = df.groupby('서울시민업종코드').sum('카드이용건수계').reset_index()
n_grp = df_2030.groupby('업종코드').sum('카드이용건수계').reset_index()
grp_top10 = n_grp.sort_values('카드이용건수계', ascending=False).iloc[:10, 0] # 10
print(grp_top10)
'''
65         편의점
68    할인점/슈퍼마켓
40       온라인거래
7         기타요식
3     결제대행(PG)
66          한식
62          통신
61       커피전문점
10     기타음/식료품
5           교통
'''

##############################
## 3. ARIMA/SARIMAX 모델에 맞게 input data 모양 변경
##############################
'''
ARIMA : 시계열 값만 포함하는 1차원 배열, 인덱스는 DatetimeIndex
        -> series, np.array

SARIMAX : 시계열 데이터와 외부 변수를 포함하는 2차원 배열, 인덱스는 DatetimeIndex
          -> dataframe
'''

# 날짜 기준 데이터
date = pd.Series(sorted(df_2030['기준년월'].unique()))
date.name = '기준년월'
len(date) # 20
date.dtype

import numpy as np

# arima input data : series(날짜 , value)
def arima_input(cat, y) : # cat : grp_top5, y : 카드이용금액계(AMT_CORR)/카드이용건수계(USECT_CORR)

    # category subset 생성
    # grp_data = grouped_data[grouped_data['서울시민업종코드'] == cat]
    grp_data = grouped_data[grouped_data['업종코드'] == cat]

    #grp_data[grp_data['카드이용금액계'] == grp_data['카드이용금액계'].max()] = np.mean(grp_data['카드이용금액계'])

    # 날짜 오름차순으로 정렬
    grp_data = grp_data.sort_values('기준년월')

    # 카테고리별 target 날짜 기준 데이터랑 merge
    merge_data = grp_data.merge(date, how='right', on='기준년월')

    # datetimeindex로 변환
    merge_data.index = pd.to_datetime(merge_data['기준년월'], format='%Y%m')

    # 시계열 데이터만 남기기
    arima_data = merge_data[y]

    # 카테고리별 subset에 관측기간 중 빈 날짜 존재 시 데이터 채우기
    if arima_data.isna().sum() > 0:
        arima_data = arima_data.interpolate(method='time')

    '''
    interpolate()

    결측치 보간(interpolation)은 결측치가 발생한 위치의 앞뒤 데이터를 사용하여 적절한 값을 추정하는 방법
    선형 보간, 시간 보간, 다항 보간 등 다양한 방법이 있으며, 상황에 따라 적절한 방법을 선택해야 한다.
    결측치가 많은 경우나 결측치가 연속으로 발생하는 경우, 보간을 통해 추정한 값이 실제 값과 크게 달라질 수 있.
    '''

    # 데이터 채운 후에도 null값 존재 시 0으로 대체
    if arima_data.isna().sum() > 0 :
      arima_data= arima_data.fillna(0) # arima_data.fillna(0, inplace=True)

    # plt.figure(figsize=(12,4))
    # plt.xlabel('날짜')
    # plt.ylabel('카드이용 금액')
    # plt.title(f'{cat} 시계열 데이터')
    # arima_data.plot()


    # if '/' in cat:
    #     cat = cat.replace('/', '_')

    # plt.savefig(r'\\\final_project\시각화/'+f'{cat}_원본데이터.png')
    # plt.show()

    return arima_data

'''
for i in grp_top5:
    arima_input(i, '카드이용금액계')
   '''
#arima1 = arima_input('편의점', '카드이용금액계')


##############################
## 4.  정상성 확인 : 아리마 모델이 정상 시계열 데이터를 가정하기 때문
##############################
'''
귀무가설: 해당 시계열은 정상(stationary) 시계열이다.
대립가설: 해당 시계열은 비정상(non-stationary) 시계열이다.

p-value <= 0.05 : 귀무가설 기각/ 대립가설 채택 -> 비정상(non-stationary) 시계열.
p-value > 0.05 : 귀무가설 채택/ 대립가설 기각 -> 정상(stationary) 시계열.
'''
def kpss_test(series, cat):

  stats, p_value, nlags, critical_values = kpss(series) # kpss검정: 통계량, pvalue,사용된 지연개수 출력   (ADF 검정)

  diff = 0
    # 비정상 시계열일 경우 차분 : 이전값과 현재값의 차이
  while p_value <= 0.05:
      series = series.diff(periods=1).iloc[1:]
      stats, p_value, nlags, critical_values = kpss(series)

      print(f"비정상(non-stationary) 시계열 데이터 입니다. 차수: {diff}")

      diff += 1

  print(f'KPSS Stat: {stats:.5f}')
  print(f'p-value: {p_value:.2f}')
  print(f'Lags: {nlags}')
  print('검증결과: 정상(stationary) 시계열 데이터 입니다.')

    # # 한글 지원 : 폰트 설정
    # plt.rcParams['font.family'] = 'Malgun Gothic'

    # # 마이너스 지원
    # plt.rcParams['axes.unicode_minus'] = False

    # # 차수 확인
    # fig, axes = plt.subplots(1, 2)
    # fig.set_size_inches(12, 4)

    # # ACF Plot(자기상관함수) : 도출된 잔차가 시간에 따라 상관성이 존재하는지 확인하는 함수, ACF 그래프에서 0에가까워지는 지점확인(MA 차수선택)
    # plot_acf(series, lags=15, ax=axes[0])
    # # 급격히 값이 감소하여 0과 가까워지는 지점 : 2 -> MA(0)

    # # PACF Plot(편자기상관함수) : 편미분을 활용하여 편미분계수를 구하는것, PACF 그래프에서 급격한 감소지점확인(AR 차수선택)
    # plot_pacf(series, lags=8, zero=False, ax=axes[1])
    # # 급격히 값이 감소하여 0과 가까워지는 지점 : 4 -> AR(2)

    # for ax in axes:
    #     ax.set_ylim(-0.5, 1.25)

    # plt.show()
    # 스케일링
    #scaled_series = (series - series.min()) / (series.max() - series.min())

  train = series[:-3]
  test = series[-3:]

    # 시계열 데이터 시각화
    # plt.figure(figsize=(12,4))
    # plt.xlabel('날짜')
    # plt.ylabel('카드이용 금액')
    # plt.title(f'{cat} 시계열 데이터')
    # series.plot()
    # plt.show()
  return series, train, test

#input_data, train, test = kpss_test(arima1, '편의점')

##############################
## 시계열 자료 성분 분석 : 추세, 계절, 불규칙
## 추세(Trend) : 시계열 데이터의 장기적 패턴
## 계절(Seasonal) : 주기적으로 반복되는 패턴
## 불규칙(Irregular) : 추세나 계절성에 의해 설명되지 않는 잔차(Residual)
##############################




##############################
## 5. arima model
##############################
'''
ARIMA 모델의 freq 인자는 주로 DatetimeIndex에서 활용되기 때문에
문자열 형태의 인덱스를 DatetimeIndex로 변경해야 함
'''

def arima_model(series, cat, s): # series:input data, cat:카테고리, s:예측기간

    warnings.filterwarnings('ignore')

    # 차수 관련 best parameter 찾기
    # p(자기회귀 차수, AR) : 과거의 값이 현재값에 얼마나 영향을 미치는지 나타냄
    # q(차분 차수, I) : 몇번 차분할것인지 표현
    # q(이동평균 차수, MA) : 과거의 오차가 현재에 얼마나 영향을 미치는지 나타냄
    p = range(0, 4)
    d = range(0, 3)
    q = range(0, 5)

    pdq = list(itertools.product(p,d,q)) # 두개 이상의 리스트끼리의 데카르트 곱을 계산하여 iterator로 반환

    aic = []
    params = []

    # 모든 조합을 테스트하고 AIC 값이 가장 낮은 모델을 찾음.
    with tqdm(total=len(pdq)) as pg:
        for i in pdq:
            pg.update(1)
            try:
                model = ARIMA(series, order=i).fit()
                aic.append(round(model.aic, 2))
                params.append(i)
            except Exception as e:
                print(f"Error for {i}: {e}")
                continue

    # 최적모델 학습 ( 최적 차수(p, d, q)를 사용하여 ARIMA 모델 학습 )
    optimal = [(params[i],j) for i,j in enumerate(aic) if j == min(aic)]

    if series.index.dtype != 'period[M]':
        series.index = series.index.to_period('M')

    arima = ARIMA(series, order=optimal[0][0], freq='M')
    arima_fit = arima.fit()

    # 향후 3달에 대한 예측치 생성
    forecast = arima_fit.forecast(steps=s)
    #forecast = (forecast * (forecast.max() - forecast.min())) + forecast.min()

    # 시각화
    # series.plot(label='real')
    # forecast.plot(color='red', linestyle='--', label='predict')
    # plt.title(f'{cat} 시계열 데이터 향후 3개월 소비금액 예측')
    # plt.xlabel('날짜')
    # plt.ylabel('카드이용 금액')

    # if '/' in cat:
    #     cat = cat.replace('/', '_')

    # plt.savefig(r'\\final_project\시각화/'+f'{cat}_예측데이터.png')

    # plt.show()
    return arima, arima_fit, forecast

#arima, arima_fit, forecast = arima_model(train, '편의점', 3)

'''
from sklearn.metrics import r2_score
r2 = r2_score(test,forecast)

'''
# 예측값 저장
pred_y = pd.DataFrame()

# 카테고리 저장
keys = []
pred_y = []

import pickle
for i in grp_top10:

    arima_data = arima_input(i, '카드이용금액계')
    input_data, train, test = kpss_test(arima_data, i)
    arima, arima_fit, forecast = arima_model(train, i, 3)
    print(arima_fit.summary())

    if '/' in i:
        i = i.replace('/', '_')

    with open(f'{i}_arima.pkl', 'wb') as model_file:
        pickle.dump(arima_fit, model_file)

      # '{i}_arima.pkl' 이라고 지정해서 "편의점_arima.pkl" 로 나옴
      # with : 사용하면 파일을 자동으로 닫아주기 때문
      # 'wb' : 쓰기 모드(w)와 바이너리 모드(b)로 열어서 이진 데이터로 저장할 수 있도록 설정합니다.
      # pickle.dump(arima_fit, model_file) : pickle.dump() 함수는 arima_fit 모델을 파일(model_file)에 저장한다

    keys.append(i)
    pred_y.append(round(forecast,3))

# type(pred_y[0])

# result = pd.DataFrame()

new_index = [f'{item}_mean' for item in grp_top10]
result = pd.DataFrame(pred_y, index=new_index)
result.to_csv('pred_y.csv')



"""

##############################
## 6. SARIMAX
##############################
def sarimax_model(series, cat, order):

    sarimax = SARIMAX(series, order=order, season_order = (1, 1, 2, 14)).fit(disp=False)
    forecast_sm = sarimax.forecast(steps=3)


    plt.figure(figsize=(10, 5))
    plt.plot(time_data['기준년월(YM)'], input_data, label='real', color='green')
    plt.plot(['2021-08', '2021-09', '2021-10'], forecast_sm, label='predict', color='red')
    plt.xticks(index[::5].strftime('%Y-%m'), rotation=45)  # 5개마다 하나씩 눈금 표시
    plt.title('SARIMAX 예측치')
    plt.legend()
    plt.show()

    # train / test 분할
    train_data = input_data[:-5]
    test_data = input_data[-5:]

    # 향후 5개월 데이터에 대한 예측
    n_predictions = 5

    sarimax = SARIMAX(train_data,
                    order=(1, 2, 1),
                    season_order = (1, 1, 2, 14)).fit(disp=False)

    forecast_sm = sarimax.forecast(steps=n_predictions)
    '''
    2021-03    8.807269e+06
    2021-04    8.927031e+06
    2021-05    9.061719e+06
    2021-06    9.202688e+06
    2021-07    9.346299e+06
    '''

    plt.figure(figsize=(10, 5))
    plt.plot(time_data['기준년월(YM)'], input_data, label='real', color='green')
    plt.plot(['2021-03', '2021-04', '2021-05', '2021-06', '2021-07'], forecast_sm, label='predict', color='red', linestyle='--')
    plt.xticks(time_data['기준년월(YM)'][::5], rotation=45)  # 5개마다 하나씩 눈금 표시
    plt.title('SARIMAX train/test 예측치')
    plt.legend()
    plt.show()

    # SARIMAX 하이퍼파라미터 튜닝
    p = range(0, 5)  # AR
    d = range(1, 3)  # 차분
    q = range(0, 4)  # MA
    m = 30           # 계절성 주기(Seasonal Trends)

    pdq = list(itertools.product(p, d, q))
    seasonal_pdq = [(x[0], x[1], x[2], m) for x in list(itertools.product(p, d, q))]

    aic = []
    params = []

    with tqdm(total = len(pdq) * len(seasonal_pdq)) as pg:
        for i in pdq:
            for j in seasonal_pdq:
                pg.update(1)
                try:
                    model = SARIMAX(train_data,
                                    order=(i),
                                    season_order = (j)).fit(disp=False)
                    aic.append(round(model.aic, 2))
                    params.append((i,j))
                except:
                    print('Error Occured..')
                    continue

    optimal = [(params[i], j) for i, j in enumerate(aic) if j == min(aic)]

    sarimax_tunned = SARIMAX(train_data,
                             order = optimal[0][0][0],
                             seasonal_order = optimal[0][0][1]).fit(disp=False)

    tunned_forecast = sarimax_tunned.forecast(steps=n_predictions)

    return sarimax, forecast



sarimax_tunned.summary()
'''
                                      SARIMAX Results
===========================================================================================
Dep. Variable:                   카드이용금액계(AMT_CORR)   No. Observations:                   62
Model:             SARIMAX(2, 2, 2)x(0, 1, [], 30)   Log Likelihood                -443.244
Date:                             Mon, 22 Jan 2024   AIC                            896.488
Time:                                     02:21:28   BIC                            903.494
Sample:                                 01-31-2016   HQIC                           898.729
                                      - 02-28-2021
Covariance Type:                               opg
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
ar.L1         -0.4783      0.205     -2.329      0.020      -0.881      -0.076
ar.L2          0.2916      0.195      1.499      0.134      -0.090       0.673
ma.L1      -1.144e-14   2.84e-14     -0.403      0.687    -6.7e-14    4.41e-14
ma.L2         -1.0000      0.176     -5.673      0.000      -1.345      -0.655
sigma2      3.751e+11    4.7e-13   7.98e+23      0.000    3.75e+11    3.75e+11
===================================================================================
Ljung-Box (L1) (Q):                   0.54   Jarque-Bera (JB):               206.49
Prob(Q):                              0.46   Prob(JB):                         0.00
Heteroskedasticity (H):               0.02   Skew:                             2.90
Prob(H) (two-sided):                  0.00   Kurtosis:                        14.47
===================================================================================
'''


'''
2021-03    8.865898e+06
2021-04    8.965491e+06
2021-05    9.127480e+06
2021-06    9.246991e+06
2021-07    9.976675e+06
'''

# 예측결과 시각화
plt.figure(figsize=(10, 5))
plt.plot(time_data['기준년월(YM)'], input_data, label='real', color='#1AA341', alpha=0.5)
plt.plot(['2021-03', '2021-04', '2021-05', '2021-06', '2021-07'], tunned_forecast, label='predict', color='#F02828')
plt.axvline(x = '2021-03', color = 'blue', label = 'After Prediction', linestyle='dotted')
plt.xticks(time_data['기준년월(YM)'][::5], rotation=45)  # 5개마다 하나씩 눈금 표시
plt.title('SARIMAX 최종 예측 결과')
plt.legend()
plt.show()



# 정상성 확인 - ACF(Auto Correlation Function), PACF(Partial Autocorrelation Function) 시각화

# 카테고리별로 선택
category_data = grouped_data[grouped_data['서울시민업종코드(UPJONG_CD)'] == 'LPG']

# 카테고리별 target 날짜랑 merge
merge_data = category_data.merge(target, how='right', on='기준년월(YM)')

# 빈 날짜에 데이터 채우기
merge_data['카드이용금액계(AMT_CORR)'] = merge_data['카드이용금액계(AMT_CORR)'].interpolate(method='time')


# 업종코드 제거하기
time_data = merge_data.drop('서울시민업종코드(UPJONG_CD)', axis=1)

# 날짜에 따른 데이터 시각화 - 추세확인
time_data['기준년월(YM)'] = [i.strftime("%Y-%m") for i in time_data['기준년월(YM)']]

plt.plot(time_data['기준년월(YM)'], time_data['카드이용금액계(AMT_CORR)'])
plt.show()

input_data = time_data['카드이용금액계(AMT_CORR)']
input_data.index = time_data['기준년월(YM)']

if input_data.isna().sum() > 0 :
    input_data.fillna(input_data.mean(), inplace=True)


# 정상성 검증
def kpss_test(series, **kw):
    stats, p_value, nlags, critical_values = kpss(series, **kw)

    print(f'KPSS Stat: {stats:.5f}')
    print(f'p-value: {p_value:.2f}')
    print(f'Lags: {nlags}')

    print(f'검증결과: {"비정상(non-stationary)" if p_value <= 0.05 else "정상(stationary)"} 시계열 데이터입니다.')

kpss_test(input_data)
'''
KPSS Stat: 0.89689
p-value: 0.01
Lags: 5
검증결과: 비정상(non-stationary) 시계열 데이터입니다.
'''

# arima model

type(input_data.index) # <class 'pandas.core.indexes.base.Index'>
'''
ARIMA 모델의 freq 인자는 주로 DatetimeIndex에서 활용되기 때문에
문자열 형태의 인덱스를 DatetimeIndex로 변경해야 함
'''
input_data.index = pd.to_datetime(input_data.index, format='%Y-%m')
input_data.index = pd.PeriodIndex(input_data.index, freq='M')
type(input_data.index) # <class 'pandas.core.indexes.datetimes.DatetimeIndex'>

warnings.filterwarnings('ignore')

p = range(0, 4)
d = range(0, 3)
q = range(0, 5)

pdq = list(itertools.product(p,d,q))

aic = []
params = []

with tqdm(total=len(pdq)) as pg:
    for i in pdq:
        pg.update(1)
        try:
            model = ARIMA(series, order=(i)).fit()
            aic.append(round(model.aic, 2))
            params.append((i))
        except:
            continue

p = range(0, 4)
d = range(0, 3)
q = range(0, 5)

pdq = list(itertools.product(p, d, q))

aic = []
params = []

with tqdm(total=len(pdq)) as pg:
    for i in pdq:
        pg.update(1)
        try:
            model = ARIMA(input_data, i).fit()
            aic.append(round(model.aic, 2))
            params.append(i)
        except Exception as e:
            print(f"Error for {i}: {e}")
            continue

input_data.info()

optimal = [(params[i],j) for i,j in enumerate(aic) if j == min(aic)]
# [((1, 2, 1), 1882.67)]

arima = ARIMA(input_data, order = optimal[0][0], freq='M').fit()
arima.summary()

input_data.info()
'''
                               SARIMAX Results
==============================================================================
Dep. Variable:      카드이용금액계(AMT_CORR)   No. Observations:             67
Model:                 ARIMA(1, 2, 1)   Log Likelihood                -938.337
Date:                Mon, 22 Jan 2024   AIC                           1882.675
Time:                        01:23:24   BIC                           1889.198
Sample:                    01-31-2016   HQIC                          1885.248
                         - 07-31-2021
Covariance Type:                  opg
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
ar.L1          0.4263      0.046      9.252      0.000       0.336       0.517
ma.L1         -0.9967      0.043    -23.070      0.000      -1.081      -0.912
sigma2      1.973e+11   2.23e-13   8.86e+23      0.000    1.97e+11    1.97e+11
===================================================================================
Ljung-Box (L1) (Q):                   1.69   Jarque-Bera (JB):              3315.48
Prob(Q):                              0.19   Prob(JB):                         0.00
Heteroskedasticity (H):               0.03   Skew:                            -4.71
Prob(H) (two-sided):                  0.00   Kurtosis:                        36.70
===================================================================================
'''

# 향후 3달에 대한 예측치 생성
forecast = arima.forecast(steps=3)
'''
2021-08    8795167.4424
2021-09    8898136.8132
2021-10    9014227.9730
'''

# 시각화
fc = forecast.copy()
fc = pd.concat([pd.Series([forecast[-1]], index=['2021-07']), fc])

index = input_data.index.append(forecast.index)




fig, ax = plt.subplots(1, 1)
fig.set_size_inches(10, 5)
fig.set_dpi(300)
ax.plot(time_data['기준년월(YM)'], input_data, color='green', label='real')
ax.plot(['2021-08', '2021-09', '2021-10'], forecast, color='red', label='predict')
xticks_loc = ax.get_xticks()
ax.set_xticks(xticks_loc[::5])  # 5개마다 하나씩 눈금 표시
ax.set_xticklabels(index[::5].strftime('%Y-%m'), rotation=45)  # 눈금에 표시할 날짜 형식 지정
plt.title('LPG 향후 3개월 소비 예측')
plt.legend()
plt.show()




# sarimax
sarimax = SARIMAX(input_data, order=(1, 2, 1), season_order = (1, 1, 2, 14)).fit(disp=False)
forecast_sm = sarimax.forecast(steps=3)

plt.figure(figsize=(10, 5))
plt.plot(time_data['기준년월(YM)'], input_data, label='real', color='green')
plt.plot(['2021-08', '2021-09', '2021-10'], forecast_sm, label='predict', color='red')
plt.xticks(index[::5].strftime('%Y-%m'), rotation=45)  # 5개마다 하나씩 눈금 표시
plt.title('SARIMAX 예측치')
plt.legend()
plt.show()

# train / test 분할
train_data = input_data[:-5]
test_data = input_data[-5:]

# 향후 5개월 데이터에 대한 예측
n_predictions = 5

sarimax = SARIMAX(train_data,
                order=(1, 2, 1),
                season_order = (1, 1, 2, 14)).fit(disp=False)

forecast_sm = sarimax.forecast(steps=n_predictions)
'''
2021-03    8.807269e+06
2021-04    8.927031e+06
2021-05    9.061719e+06
2021-06    9.202688e+06
2021-07    9.346299e+06
'''

test_data
'''
2021-03    8722976.0
2021-04    8722976.0
2021-05    8722976.0
2021-06    8722976.0
2021-07    8722976.0
'''

plt.figure(figsize=(10, 5))
plt.plot(time_data['기준년월(YM)'], input_data, label='real', color='green')
plt.plot(['2021-03', '2021-04', '2021-05', '2021-06', '2021-07'], forecast_sm, label='predict', color='red', linestyle='--')
plt.xticks(time_data['기준년월(YM)'][::5], rotation=45)  # 5개마다 하나씩 눈금 표시
plt.title('SARIMAX train/test 예측치')
plt.legend()
plt.show()

# SARIMAX 하이퍼파라미터 튜닝
p = range(0, 5)  # AR
d = range(1, 3)  # 차분
q = range(0, 4)  # MA
m = 30           # 계절성 주기(Seasonal Trends)

pdq = list(itertools.product(p, d, q))
seasonal_pdq = [(x[0], x[1], x[2], m) for x in list(itertools.product(p, d, q))]

aic = []
params = []

with tqdm(total = len(pdq) * len(seasonal_pdq)) as pg:
    for i in pdq:
        for j in seasonal_pdq:
            pg.update(1)
            try:
                model = SARIMAX(train_data,
                                order=(i),
                                season_order = (j)).fit(disp=False)
                aic.append(round(model.aic, 2))
                params.append((i,j))
            except:
                print('Error Occured..')
                continue

optimal = [(params[i], j) for i, j in enumerate(aic) if j == min(aic)]

sarimax_tunned = SARIMAX(train_data,
                         order = optimal[0][0][0],
                         seasonal_order = optimal[0][0][1]).fit(disp=False)

sarimax_tunned.summary()
'''
                                      SARIMAX Results
===========================================================================================
Dep. Variable:                   카드이용금액계(AMT_CORR)   No. Observations:                   62
Model:             SARIMAX(2, 2, 2)x(0, 1, [], 30)   Log Likelihood                -443.244
Date:                             Mon, 22 Jan 2024   AIC                            896.488
Time:                                     02:21:28   BIC                            903.494
Sample:                                 01-31-2016   HQIC                           898.729
                                      - 02-28-2021
Covariance Type:                               opg
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
ar.L1         -0.4783      0.205     -2.329      0.020      -0.881      -0.076
ar.L2          0.2916      0.195      1.499      0.134      -0.090       0.673
ma.L1      -1.144e-14   2.84e-14     -0.403      0.687    -6.7e-14    4.41e-14
ma.L2         -1.0000      0.176     -5.673      0.000      -1.345      -0.655
sigma2      3.751e+11    4.7e-13   7.98e+23      0.000    3.75e+11    3.75e+11
===================================================================================
Ljung-Box (L1) (Q):                   0.54   Jarque-Bera (JB):               206.49
Prob(Q):                              0.46   Prob(JB):                         0.00
Heteroskedasticity (H):               0.02   Skew:                             2.90
Prob(H) (two-sided):                  0.00   Kurtosis:                        14.47
===================================================================================
'''

tunned_forecast = sarimax_tunned.forecast(steps=n_predictions)
'''
2021-03    8.865898e+06
2021-04    8.965491e+06
2021-05    9.127480e+06
2021-06    9.246991e+06
2021-07    9.976675e+06
'''

# 예측결과 시각화
plt.figure(figsize=(10, 5))
plt.plot(time_data['기준년월(YM)'], input_data, label='real', color='#1AA341', alpha=0.5)
plt.plot(['2021-03', '2021-04', '2021-05', '2021-06', '2021-07'], tunned_forecast, label='predict', color='#F02828')
plt.axvline(x = '2021-03', color = 'blue', label = 'After Prediction', linestyle='dotted')
plt.xticks(time_data['기준년월(YM)'][::5], rotation=45)  # 5개마다 하나씩 눈금 표시
plt.title('SARIMAX 최종 예측 결과')
plt.legend()
plt.show()


# 시계열 성분 분해

"""

'''

for category in grouped_data['서울시민업종코드(UPJONG_CD)'].unique():
    category_data = grouped_data[grouped_data['서울시민업종코드(UPJONG_CD)'] == category]

    category_data = category_data.interpolate()
    category_data.index = [i.strftime("%Y-%m") for i in category_data['기준년월(YM)']]
    category_data.drop(['서울시민업종코드(UPJONG_CD)', '기준년월(YM)'], axis=1, inplace=True)

    fig, axes = plt.subplots(1, 2)
    fig.set_size_inches(12, 4)

    # ACF Plot
    plot_acf(category_data, lags=30, ax=axes[0])

    # PACF Plot
    plot_pacf(category_data, lags=10, zero=False, ax=axes[1])

    for ax in axes:
        ax.set_ylim(-0.5, 1.25)
    plt.show()

from fbprophet import Prophet
# SARIMA 모델 적용 예시
def fit_sarima(data):
    model = SARIMAX(data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))
    result = model.fit(disp=False)
    return result

# Prophet 모델 적용 예시
def fit_prophet(data):
    model = Prophet()
    model.fit(data)
    return model

# 각 소분류에 대해 모델 훈련 및 예측
predictions = {}

for category in grouped_data['서울시민업종코드(UPJONG_CD)'].unique():
    category_data = grouped_data[grouped_data['서울시민업종코드(UPJONG_CD)'] == category]

    #
    # 카테고리 별 빈 날짜에 데이터 넣기
    category_data = category_data.interpolate()

    #
    if category_data.isna().sum() == 0:

        # SARIMA 모델 사용
        sarima_model = fit_sarima(category_data['카드이용금액계(AMT_CORR)'])
        sarima_forecast = sarima_model.get_forecast(steps=12)  # 예측 기간: 12개월

        # Prophet 모델 사용
        #prophet_model = fit_prophet(category_data.rename(columns={'기준년월(YM)': 'ds', '카드이용금액계(AMT_CORR)': 'y'}))
        #future = prophet_model.make_future_dataframe(periods=12, freq='M')
        #prophet_forecast = prophet_model.predict(future)

        predictions[category] = {'SARIMA': sarima_forecast.predicted_mean.values} #'Prophet': prophet_forecast.yhat.values}


# 시각화
import matplotlib.pyplot as plt

# 예측 결과 시각화 예시
def plot_predictions(category, sarima_preds, prophet_preds):
    plt.figure(figsize=(10, 6))
    plt.plot(grouped_data[(grouped_data['소분류'] == category) & (grouped_data['월'] < '2023-01')]['월'],
             grouped_data[(grouped_data['소분류'] == category) & (grouped_data['월'] < '2023-01')]['소비금액'],
             label='실제 소비')

    plt.plot(grouped_data[(grouped_data['소분류'] == category)]['월'], sarima_preds, label='SARIMA 예측')
    plt.plot(grouped_data[(grouped_data['소분류'] == category)]['월'], prophet_preds, label='Prophet 예측')

    plt.title(f'월별 소비 예측 - {category}')
    plt.xlabel('월')
    plt.ylabel('소비금액')
    plt.legend()
    plt.show()

# 예측 결과 시각화
for category, preds in predictions.items():
    plot_predictions(category, preds['SARIMA'], preds['Prophet'])

'''

from sklearn.metrics import mean_squared_error
import numpy as np

# RMSE 계산
rmse = round(np.sqrt(mean_squared_error(test, forecast)),2)
print(f'RMSE: {rmse}')

# MAE 계산
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(test, forecast)
print(f'MAE: {mae}')

# RMSE: 8009.68
# MAE: 6539.0

result

"""### 3. 카드 크롤링 : 실행은 쥬피터 노트북에서 , 오류나면 엣지드라이버 최신버전을 다시 설치하기"""

from selenium import webdriver # driver
from selenium.webdriver.edge.service import Service # Edge 서비스
from webdriver_manager.microsoft import EdgeChromiumDriverManager # 엣지드라이버 관리자
import time
from selenium.webdriver.common.by import By

# 카드 혜택 페이지 열기
e_driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()))
e_driver.get('https://card-search.naver.com/list?ptn=1&sortMethod=ri&bizType=CPC')


# 스크롤 내려서 더보기 버튼
last_height = e_driver.execute_script("return document.body.scrollHeight") #현재 스크롤 높이 계산

while True: # 무한반복
    # 브라우저 끝까지 스크롤바 내리기
    e_driver.execute_script("window.scrollTo(5, document.body.scrollHeight);")

    time.sleep(2) # 2초 대기 - 화면 스크롤 확인

    # 화면 갱신된 화면의 스크롤 높이 계산
    new_height = e_driver.execute_script("return document.body.scrollHeight")

    # 새로 계산한 스크롤 높이와 같으면 stop
    if new_height == last_height:
        try: # [결과 더보기] : 없는 경우 - 예외처리
            e_driver.find_element(By.CLASS_NAME, "more").click() # [결과 더보기] 버튼 클릭
        except:
            break
    last_height = new_height # 새로 계산한 스크롤 높이로 대체

"""### 4. 카드혜택 크롤링 : 쥬피터노트북에서 실행"""

from selenium import webdriver # driver
from selenium.webdriver.edge.service import Service # Edge 서비스
from webdriver_manager.microsoft import EdgeChromiumDriverManager # 엣지드라이버 관리자

from selenium.webdriver.common.by import By # By.NAME
import time

from urllib.request import urlopen
from bs4 import BeautifulSoup

import pandas as pd
import os
from urllib.request import urlretrieve # server image save


# 카드 혜택 상세 페이지 url 및 혜택 추출

# 1. driver 객체 생성
# driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()))

edge_driver_path = "C:/Users/HJO/.wdm/drivers/edgedriver/win64/132.0.2957.140/msedgedriver.exe"
service = Service(edge_driver_path)
e_driver = webdriver.Edge(service=service)


# 2. 카드 혜택 페이지 열기
url = 'https://card-search.naver.com/list?ptn=1&sortMethod=ri&bizType=CPC'
e_driver.get(url)


# 3. 스크롤바 내리기
# 현재 스크롤 높이 계산
last_height = driver.execute_script("return document.body.scrollHeight")

while True: # 무한반복
    # 브라우저 끝까지 스크롤바 내리기
    driver.execute_script("window.scrollTo(5, document.body.scrollHeight);")

    # 2초 대기 - 화면 스크롤 확인
    time.sleep(2)

    # 화면 갱신된 화면의 스크롤 높이 계산
    new_height = driver.execute_script("return document.body.scrollHeight")

    # 새로 계산한 스크롤 높이와 같으면 stop
    if new_height == last_height:
        try:
            driver.find_element(By.CLASS_NAME, "more").click() # [결과 더보기] 버튼 클릭
        except:
            break  # [결과 더보기] : 없는 경우 - while문 실행 종료
    last_height = new_height # 새로 계산한 스크롤 높이로 대체


# 4. 카드 혜택 상세페이지 urls 추출
# 선택자 지정 - locator = CSS_SELECTOR
links = driver.find_elements(By.CSS_SELECTOR, 'a.anchor')
print('수집 a_tags 개수 =', len(links))

# a태그에서 url만 추출
urls = []

for a in links :
    # a태그의 href가 가지고 있는 url 추출 -> urls 리스트에 저장
    urls.append(a.get_attribute('href'))

# 중복 url 삭제
card_url = list(set(urls)) # 중복 url  삭제

# 'cardAdId' 포함 url만 추출 : 카드 상세페이지
card_url = [i for i in card_url if 'cardAdId' in i]
print(len(card_url)) # 312


# 5. 카드 이름, 혜택 카테고리, 상세내용 추출
'''
# [방법 1] <카드별 카테고리, 상세혜택 합쳐서>
# 카드 이름, [혜택1, 혜택2, ...], [상세혜택1, 상세혜택2, ...]
c_name = []
category = []
benef = []

for url in card_url:
    byte_data = urlopen(url).read()
    text_data = byte_data.decode("utf-8") # 디코딩
    html = BeautifulSoup(text_data, 'html.parser') # html source 파싱

    name = html.select_one("b.txt").text
    cat = [i.text for i in html.select("b.text")]
    ben = [i.text for i in html.select("i.desc")]

    c_name.append(name)
    category.append(cat)
    benef.append(ben)

data = {'name':c_name, 'category':category, 'benefits':benef}
new_bene = pd.DataFrame(data)


driver.close() # 창 닫기
'''

# [방법 2] 카드 혜택 저장 df
benefits = pd.DataFrame(columns=['name', 'category', 'benefits', 'img'])

for i in range(len(card_url)):
    # 카드 상세페이지 접속 및 html 문서 파싱
    url = card_url[i]
    byte_data = urlopen(url).read()
    text_data = byte_data.decode("utf-8") # 디코딩
    html = BeautifulSoup(text_data, 'html.parser') # html source 파싱

    # 카드 이름, 혜택 카테고리, 상세 내용, img 내용 추출
    card_name = html.select_one("b.txt").text
    cat = [i.text for i in html.select("b.text")]
    bene = html.select("i.desc")
    img_tag = html.find("img")  # 이미지 태그 추출
    img_str = str(img_tag) if img_tag else None

    # 카드이름, 카테고리, 상세내용 row형태로 저장
    rows = []

    for category, details in zip(cat, bene):
        conts = details.contents
        texts = [element for element in conts if element.name != 'br']
        for detail in texts:
            rows.append({'name': card_name, 'category': category, 'benefits': detail, 'img':img_str})

    # rows -> df 변환
    df = pd.DataFrame(rows)

    # benefits에 df concat
    benefits = pd.concat([benefits, df], ignore_index=True)

'''
# 7. 카드 혜택 csv 파일 저장
benefits.to_csv('카드혜택.csv', encoding='UTF-8', index=False)
dir(benefits)
'''

"""### 5. 카드추천 프로그램"""

#############################################################################
### 1. vec1 만들기 ###############################################################
#############################################################################

import pandas as pd
import numpy as np

# 소비내역 불러오기
# csv_file_path = r'C:\Study\Python\final_proj\data/통합소비내역5_최종.csv'
# csv_file_path = r'C:\Study\Python\final_proj\data/통합소비내역5_최종.csv'
file = pd.read_csv('통합소비내역5_최종.csv', encoding='cp949')

consump = file.copy() # 카피데이터

# 원본데이터에서 추출한 소비건수 기준 top10 카테고리 적용
top10 = ['편의점', '할인점/슈퍼마켓', '온라인거래', '기타요식', '결제대행(PG)', '한식', '통신', '커피전문점', '기타음/식료품', '교통']
consump = consump[consump['소분류'].isin(top10)]

# 카드혜택 카테고리 & 소비 카테고리 일치시키기
change = {'카페/베이커리': ['커피전문점', '제과점'],
        '쇼핑': ['생활잡화', '의복/의류', '패션/잡화','백화점'],
        '대중교통': ['교통'],
        '대형마트': ['할인점/슈퍼마켓', '슈퍼마켓', '기타음/식료품'],
        '의료': ['일반병원', '치과병원', '종합병원', '약국'],
        '영화': ['영화/공연'],
        '뷰티': ['화장품', '미용서비스'],
        '외식': ['한식', '일식', '양식', '중식', '패스트푸드', '기타요식'],
        '간편결제': ['결제대행(PG)', '온라인거래']}


# 소비 카테고리 변경
for c in consump.index:
    for i, j in change.items():
        if consump.loc[c,'소분류'] in j:
            consump.loc[c,'소분류'] = i

consump['소분류'].unique()
# ['간편결제', '외식', '대중교통', '편의점', '대형마트', '카페/베이커리', '통신']

# 소분류 빈도 확인
dt_cnt = consump['소분류'].value_counts().sort_index()
'''
간편결제       506
대중교통       561
대형마트        83
외식         570
카페/베이커리    478
통신          41
편의점        356
'''


# 소분류별 금액 합계 구하기
consump['금액'] = consump['금액'].apply(abs)
tot_price = consump.groupby('소분류')['금액'].sum().sort_index()
'''
간편결제       10991632
대중교통        6542023
대형마트        2153300
외식          8904517
카페/베이커리     3237270
통신          1727050
편의점         1457940
'''


# 소분류별 결제 건수, 금액 총합 데이터프레임
top_df = pd.DataFrame({'count':dt_cnt, 'tot_price':tot_price})

# 소분류 빈도, 금액 총합으로 정렬
sorted_top = top_df.sort_values(by=['count', 'tot_price'], ascending=False)
'''
         count  tot_price
소분류
외식         570    8904517
대중교통       561    6542023
간편결제       506   10991632
카페/베이커리    478    3237270
편의점        356    1457940
대형마트        83    2153300
통신          41    1727050
'''

#----------------------------------------------------------------------------
# 카드 데이터 불러오기
# card_df = pd.read_csv(r'C:\Study\Python\final_proj\data/카드혜택_2_최종.csv')

card_df = pd.read_csv('카드혜택_2_최종.csv' , encoding='cp949')

# 월별 업종빈도의 가중치(CF weight) : 1 + log(업종빈도)
cf_w = [np.log(i) for i in sorted_top['count']]

# 상품빈도
cnt = []

for i in sorted_top.index:
    n = 0
    for j in card_df['category'] :
        if i in j :
            n += 1
    cnt.append(n)


# 역상품 빈도값(IGF) : log(카드 상품 수/상품 빈도)
# 상품 빈도가 0일 경우 IFG = 0 대체
IGF = [0 if i == 0 else np.log(312/ i) for i in cnt]

# 가중치 값(weight) : CFweight * IGF
w = np.array(IGF) * np.array(cf_w)

# 쿼리길이 구하기 sqrt((weight1)^2 + (weight2)^2 + ...)
query_len = np.sqrt(sum(w**2)) # 16.86475823021001

# 정규화값
norm = w / query_len

# vec1 생성
vec1 = pd.DataFrame({'cf_w':cf_w, 'IGF':IGF, 'w':w, 'norm':norm})


#############################################################################
### 2. vec2 만들기 ###############################################################
#############################################################################
# 카드 테이블 (벡터테이블2)
c_name = card_df['name']

# 카드 벡터테이블 dict형 저장 - key:카드이름, values:벡터테이블
vec2 = {}

# 모든 카드에 대한 벡터테이블 생성
for n in range(len(card_df)):

    # 카드 하나에 대한 cf_w 값 생성 : 카테고리 존재 1, 미존재 0
    rows = []

    for t in sorted_top.index:
        if t in card_df['category'][n]:
            rows.append({'cat' : t, 'cf_w' : 1})
        else :
            rows.append({'cat' : t, 'cf_w' : 0})

    df = pd.DataFrame(rows)

    # 쿼리길이 = sqrt(sum(w1^2 + ... wn^2))
    query_len2 = np.sqrt(sum([i ** 2 for i in df['cf_w']]))

    # 정규화 값(normalization) : 가중치/쿼리길이
    df['norm'] = [i/query_len2 for i in df['cf_w']]

    # norm값이 NaN일 경우 0으로 대체
    df['norm'].fillna(0, inplace=True)

    vec2[c_name[n]] = df


# 유사도 값 구하기
sim = []

for i in range(len(c_name)):
    norm2 = np.array(vec2[c_name[i]]['norm'])
    sim.append(sum(norm * norm2))

simm = pd.DataFrame({'simm':sim})

simm['c_name'] = c_name

# 상위 5개 카드 추천
result = simm.sort_values('simm', ascending=False).head(5)

print(result)
'''
         simm           c_name
174  0.935823   KB국민 청춘대로 톡톡카드
305  0.935823        스타벅스 현대카드
24   0.927277     KB국민 톡톡Pay카드
23   0.884731  NH농협 올바른FLEX 카드
262  0.884731   KB국민 알뜰교통플러스카드
'''

print(result)



"""### 코사인 유사도 예시"""

# 좋아하는 영화를 입력하면, 해당 영화의 줄거리와 유사한 줄거리의 영화를 찾아서 추천하는 시스템
# TF-IDF는 특정 단어가 한 문서 내에서 얼마나 중요한지를 수치화하는 통계적 가중치
# 코사인 유사도는 두 벡터가 가리키는 방향이 얼마나 유사한지를 측정하는 방법
# 다운로드 링크 : https://www.kaggle.com/rounakbanik/the-movies-dataset

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

data = pd.read_csv('movies_metadata.csv', low_memory=False)
data.head(2) # 45466 데이터

data = data.head(20000)
data

# TF-IDF를 연산할 때 데이터에 Null 값이 들어있으면 에러가 발생함
# overview 열을 가지고 영화제목 추천하므로
print('overview 열의 결측값의 수:',data['overview'].isnull().sum()) # overview 열의 결측값의 수: 135

# 결측값을 빈 값으로 대체
data['overview'] = data['overview'].fillna('')
data['overview'].isnull().sum() # np.int64(0)

tfidf = TfidfVectorizer(stop_words='english') # 영어제외
tfidf_matrix = tfidf.fit_transform(data['overview']) # overview 데이터를 TF-IDF 모델로 학습하고, TF-IDF 행렬로 변환(TF-IDF점수로 표현)
print('TF-IDF 행렬의 크기(shape) :',tfidf_matrix.shape)
# TF-IDF 행렬의 크기(shape) : (20000, 47487)
# 행: 분석된 문서의 수, 즉 영화 개요의 총 개수입니다.
# 열: 불용어를 제외한 고유 단어의 총 개수입니다.

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
print('코사인 유사도 연산 결과 :',cosine_sim.shape)
# 코사인 유사도 연산 결과 : (20000, 20000)
# cosine_sim[0][1]은 첫 번째 영화와 두 번째 영화 사이의 코사인 유사도 점수
# tfidf_matrix이 2번 사용한 이유 : 모든 문서 쌍 간의 유사도를 계산
# 첫번째 tfidf_matrix : 기준이 되는 문서들
# 두번째 tfidf_matrix : 비교대상이 되는 문서들

# 영화제목 입력하면 그에 맞는 인덱스 출력
title_to_index = dict(zip(data['title'], data.index))

# 영화 제목 Father of the Bride Part II의 인덱스를 리턴
idx = title_to_index['Father of the Bride Part II']
print(idx) # 4라는 인덱스 출력

def get_recommendations(title, cosine_sim=cosine_sim):
    # 선택한 영화의 타이틀로부터 해당 영화의 인덱스를 받아온다.
    idx = title_to_index[title] # 영화에 맞는 인덱스 출력

    # 해당 영화와 모든 영화와의 유사도를 가져온다.
    sim_scores = list(enumerate(cosine_sim[idx]))
    # (0, 1.0), (1, 0.54), (2, 0.78) 와 같은 쌍들을 생성 -> 첫 번째 숫자는 영화 인덱스이고, 두 번째 숫자는 유사도 점수

    # 유사도에 따라 영화들을 정렬한다.
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    # x[1] 을 기준으로 정렬
    # x[0] 은 데이터셋에 영화가 저장된 순서를 나타내는 고유한 번호

    # 가장 유사한 10개의 영화를 받아온다.
    sim_scores = sim_scores[1:11]

    # 가장 유사한 10개의 영화의 인덱스를 얻는다.
    movie_indices = [idx[0] for idx in sim_scores]

    # 가장 유사한 10개의 영화의 제목을 리턴한다.
    return data['title'].iloc[movie_indices]

get_recommendations('The Dark Knight Rises')